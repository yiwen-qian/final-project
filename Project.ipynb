{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "32f8ca24",
   "metadata": {},
   "source": [
    "# IEOR 4501 Final Project: Understanding Hired Rides in NYC\n",
    "\n",
    "#### Contributors: Joy Ren(jr4154), Yiwen Qian(yq2346)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2f75fd94",
   "metadata": {},
   "source": [
    "## Project Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "66dcde05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# all import statements needed for the project\n",
    "\n",
    "import math\n",
    "from math import sin, cos, sqrt, atan2, radians\n",
    "import os\n",
    "import warnings\n",
    "import re\n",
    "import bs4\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import requests\n",
    "import sqlalchemy as db\n",
    "import geopandas as gpd\n",
    "from matplotlib.figure import Figure\n",
    "import matplotlib.animation as animation\n",
    "from IPython.display import HTML\n",
    "from keplergl import KeplerGl\n",
    "from matplotlib.backends.backend_agg import FigureCanvasAgg\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3f1242c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# any constants need; some have been added for you, and \n",
    "# some you need to fill in\n",
    "\n",
    "TAXI_URL = \"https://www1.nyc.gov/site/tlc/about/tlc-trip-record-data.page\"\n",
    "\n",
    "TAXI_ZONES_DIR = \"taxi_zones\"\n",
    "TAXI_ZONES_SHAPEFILE = f\"{TAXI_ZONES_DIR}/taxi_zones.shp\"\n",
    "UBER_DATA = \"uber_rides_sample.csv\"\n",
    "WEATHER_CSV_DIR = \"weather\"\n",
    "\n",
    "CRS = 4326  # coordinate reference system\n",
    "\n",
    "# (lat, lon)\n",
    "NEW_YORK_BOX_COORDS = ((40.560445, -74.242330), (40.908524, -73.717047))\n",
    "LGA_BOX_COORDS = ((40.763589, -73.891745), (40.778865, -73.854838))\n",
    "JFK_BOX_COORDS = ((40.639263, -73.795642), (40.651376, -73.766264))\n",
    "EWR_BOX_COORDS = ((40.686794, -74.194028), (40.699680, -74.165205))\n",
    "\n",
    "DATABASE_URL = \"sqlite:///project.db\"\n",
    "DATABASE_SCHEMA_FILE = \"schema.sql\"\n",
    "QUERY_DIRECTORY = \"queries\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "d6601633",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make sure the QUERY_DIRECTORY exists\n",
    "try:\n",
    "    os.mkdir(QUERY_DIRECTORY)\n",
    "except Exception as e:\n",
    "    if e.errno == 17:\n",
    "        # the directory already exists\n",
    "        pass\n",
    "    else:\n",
    "        raise"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "26ad10ea",
   "metadata": {},
   "source": [
    "## Part 1: Data Preprocessing\n",
    "In Part 1, we downloaded Parquet files, cleaning and filtering for the relevant data, filling in missing data, and generating samples of these raw datasets.\n",
    "Our processing stages can be illustrated as:\n",
    "* 1. Load the Taxi Zones by x and y coordinates.\n",
    "* 2. Calculate distance using latitude and longtitude.\n",
    "* 3. Use `bs4` and `requests` module to parse html, then we can process Yellow Taxi data of NYC.\n",
    "* 4. Processing Uber Data.\n",
    "* 5. Processing Weather Data.\n",
    "* 6. Save the cleaned data for further analyis. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0d53e24",
   "metadata": {},
   "source": [
    "### Load Taxi Zones"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "58708809",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_taxi_zones(shapefile):\n",
    "    return gpd.read_file(shapefile)     #read shapefile"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e4622327",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>OBJECTID</th>\n",
       "      <th>Shape_Leng</th>\n",
       "      <th>Shape_Area</th>\n",
       "      <th>zone</th>\n",
       "      <th>LocationID</th>\n",
       "      <th>borough</th>\n",
       "      <th>geometry</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>0.116357</td>\n",
       "      <td>0.000782</td>\n",
       "      <td>Newark Airport</td>\n",
       "      <td>1</td>\n",
       "      <td>EWR</td>\n",
       "      <td>POLYGON ((933100.918 192536.086, 933091.011 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>0.433470</td>\n",
       "      <td>0.004866</td>\n",
       "      <td>Jamaica Bay</td>\n",
       "      <td>2</td>\n",
       "      <td>Queens</td>\n",
       "      <td>MULTIPOLYGON (((1033269.244 172126.008, 103343...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>0.084341</td>\n",
       "      <td>0.000314</td>\n",
       "      <td>Allerton/Pelham Gardens</td>\n",
       "      <td>3</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>POLYGON ((1026308.770 256767.698, 1026495.593 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>0.043567</td>\n",
       "      <td>0.000112</td>\n",
       "      <td>Alphabet City</td>\n",
       "      <td>4</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((992073.467 203714.076, 992068.667 20...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>0.092146</td>\n",
       "      <td>0.000498</td>\n",
       "      <td>Arden Heights</td>\n",
       "      <td>5</td>\n",
       "      <td>Staten Island</td>\n",
       "      <td>POLYGON ((935843.310 144283.336, 936046.565 14...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>258</th>\n",
       "      <td>259</td>\n",
       "      <td>0.126750</td>\n",
       "      <td>0.000395</td>\n",
       "      <td>Woodlawn/Wakefield</td>\n",
       "      <td>259</td>\n",
       "      <td>Bronx</td>\n",
       "      <td>POLYGON ((1025414.782 270986.139, 1025138.624 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>259</th>\n",
       "      <td>260</td>\n",
       "      <td>0.133514</td>\n",
       "      <td>0.000422</td>\n",
       "      <td>Woodside</td>\n",
       "      <td>260</td>\n",
       "      <td>Queens</td>\n",
       "      <td>POLYGON ((1011466.966 216463.005, 1011545.889 ...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>260</th>\n",
       "      <td>261</td>\n",
       "      <td>0.027120</td>\n",
       "      <td>0.000034</td>\n",
       "      <td>World Trade Center</td>\n",
       "      <td>261</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((980555.204 196138.486, 980570.792 19...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>261</th>\n",
       "      <td>262</td>\n",
       "      <td>0.049064</td>\n",
       "      <td>0.000122</td>\n",
       "      <td>Yorkville East</td>\n",
       "      <td>262</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>MULTIPOLYGON (((999804.795 224498.527, 999824....</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>262</th>\n",
       "      <td>263</td>\n",
       "      <td>0.037017</td>\n",
       "      <td>0.000066</td>\n",
       "      <td>Yorkville West</td>\n",
       "      <td>263</td>\n",
       "      <td>Manhattan</td>\n",
       "      <td>POLYGON ((997493.323 220912.386, 997355.264 22...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>263 rows Ã— 7 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     OBJECTID  Shape_Leng  Shape_Area                     zone  LocationID  \\\n",
       "0           1    0.116357    0.000782           Newark Airport           1   \n",
       "1           2    0.433470    0.004866              Jamaica Bay           2   \n",
       "2           3    0.084341    0.000314  Allerton/Pelham Gardens           3   \n",
       "3           4    0.043567    0.000112            Alphabet City           4   \n",
       "4           5    0.092146    0.000498            Arden Heights           5   \n",
       "..        ...         ...         ...                      ...         ...   \n",
       "258       259    0.126750    0.000395       Woodlawn/Wakefield         259   \n",
       "259       260    0.133514    0.000422                 Woodside         260   \n",
       "260       261    0.027120    0.000034       World Trade Center         261   \n",
       "261       262    0.049064    0.000122           Yorkville East         262   \n",
       "262       263    0.037017    0.000066           Yorkville West         263   \n",
       "\n",
       "           borough                                           geometry  \n",
       "0              EWR  POLYGON ((933100.918 192536.086, 933091.011 19...  \n",
       "1           Queens  MULTIPOLYGON (((1033269.244 172126.008, 103343...  \n",
       "2            Bronx  POLYGON ((1026308.770 256767.698, 1026495.593 ...  \n",
       "3        Manhattan  POLYGON ((992073.467 203714.076, 992068.667 20...  \n",
       "4    Staten Island  POLYGON ((935843.310 144283.336, 936046.565 14...  \n",
       "..             ...                                                ...  \n",
       "258          Bronx  POLYGON ((1025414.782 270986.139, 1025138.624 ...  \n",
       "259         Queens  POLYGON ((1011466.966 216463.005, 1011545.889 ...  \n",
       "260      Manhattan  POLYGON ((980555.204 196138.486, 980570.792 19...  \n",
       "261      Manhattan  MULTIPOLYGON (((999804.795 224498.527, 999824....  \n",
       "262      Manhattan  POLYGON ((997493.323 220912.386, 997355.264 22...  \n",
       "\n",
       "[263 rows x 7 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "load_taxi_zones(TAXI_ZONES_SHAPEFILE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "8d04c726",
   "metadata": {},
   "outputs": [],
   "source": [
    "def lookup_coords_for_taxi_zone_id(zone_loc_id):\n",
    "    zones= load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "    centroid= zones[zones['LocationID'] == zone_loc_id].centroid.values[0]\n",
    "    return (centroid.y, centroid.x)        # return the centroid coordinates"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32074561",
   "metadata": {},
   "source": [
    "### Calculate distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "4cbbe6cc",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_coords(from_coord, to_coord):\n",
    "    ## \"from_coord\": columns 'pickup_latitude' and 'pickup_longitude' from datarame;\n",
    "    ## \"to_coord\":  columns 'dropoff_latitude' and 'dropoff_longitude' from datarame;\n",
    "    \n",
    "    R = 6373.0  # approximate radius of earth in km    \n",
    "    lat1, lon1 = from_coord\n",
    "    lat2, lon2 = to_coord\n",
    "\n",
    "    dlat = radians(lat2-lat1)\n",
    "    dlon = radians(lon2-lon1)\n",
    "    \n",
    "    ## Calculate the distance between two coordinates in kilometers using the Haversine formula\n",
    "    a = sin(dlat/2) ** 2 + cos(radians(lat1)) * cos(radians(lat2)) * (sin(dlon/2) ** 2 )\n",
    "    coord_distance = 2 * R * atan2(math.sqrt(a), sqrt(1-a))\n",
    "    \n",
    "    return coord_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "7480c312",
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_distance_with_zones(from_zone, to_zone):\n",
    "    ##Calculate the distance between two taxi zones in kilometers using the centroid of each zone\n",
    "    \n",
    "    from_zone_coord = lookup_coords_for_taxi_zone_id(from_zone)\n",
    "    to_zone_coord = lookup_coords_for_taxi_zone_id(to_zone)   # get two coordinates from two zone ids\n",
    "    \n",
    "    return calculate_distance_with_coords(from_zone_coord, to_zone_coord)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "03f30ee6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_distance_column(dataframe):               \n",
    "    distance=[]\n",
    "    for index, row in dataframe.iterrows():\n",
    "        from_coord=(row['pickup_latitude'],row['pickup_longitude'])\n",
    "        to_coord=(row['dropoff_latitude'],row['dropoff_longitude'])\n",
    "        distance.append(calculate_distance_with_coords(from_coord,to_coord))  \n",
    "    dataframe= pd.concat([dataframe,pd.DataFrame(distance,columns=['distance'])], axis=1)\n",
    "    return dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93daa717",
   "metadata": {},
   "source": [
    "### Process Taxi Data\n",
    "Here, We want to obtain the Yellow Taxi Data January 2009 through June 2015, then do the cleaning and combining to dataframes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "1dd682b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_urls_from_taxi_page(taxi_page):\n",
    "    # Get the HTML content of the page\n",
    "    res = requests.get(taxi_page)\n",
    "    soup = bs4.BeautifulSoup(res.content, 'html.parser')\n",
    "    # Find all <a> tags with href attribute\n",
    "    urls = [a['href'] for a in soup.find_all('a', href=re.compile(\".*(2009|201[0-4]|2015-0[1-6]).*\"))]\n",
    "    return urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "35a60f70",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2015-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/fhv_tripdata_2015-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2014-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2013-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2013-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2013-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2013-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/green_tripdata_2013-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-12.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-01.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-02.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-03.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-04.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-05.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-06.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-07.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-08.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-09.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-10.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-11.parquet',\n",
       " 'https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-12.parquet']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "all_urls=get_all_urls_from_taxi_page(TAXI_URL)\n",
    "all_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "cbd0d198",
   "metadata": {},
   "outputs": [],
   "source": [
    "def filter_taxi_parquet_urls(all_urls):\n",
    "    \"\"\"\n",
    "    Given a list of URLs, filters out the URLs that do not contain yellow taxi data parquet files.\n",
    "    \"\"\"\n",
    "    parquet_urls = []\n",
    "    for url in all_urls:\n",
    "        if 'yellow_tripdata' in url:\n",
    "            parquet_urls.append(url)\n",
    "    return parquet_urls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "2f40130a",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_month(url):\n",
    "    \"\"\"\n",
    "    clean the data, and return a pandas DataFrame.\n",
    "    \"\"\"          \n",
    "\n",
    "    df = pd.read_parquet(url)\n",
    "    df = df[['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'PULocationID', 'DOLocationID', 'fare_amount']]\n",
    "    df = df.dropna()\n",
    "\n",
    "\n",
    "    \n",
    "    zone=load_taxi_zones(TAXI_ZONES_SHAPEFILE)\n",
    "    zone = zone.to_crs(4326)\n",
    "    zone['latitude'] = zone.geometry.centroid.y\n",
    "    zone['longitude'] = zone.geometry.centroid.x\n",
    "    \n",
    "\n",
    "    # remove unnecessary columns \n",
    "    # create mapping methods for latitude & longitude\n",
    "    # aim to create lat & longitude which do not exist in some taxi data\n",
    "    zone = zone[['LocationID', 'longitude', 'latitude', 'zone', 'borough']]\n",
    "    lat_map = dict(zip(zone['LocationID'], zone['latitude']))\n",
    "    lon_map = dict(zip(zone['LocationID'], zone['longitude']))\n",
    "    \n",
    "    \n",
    "    # process dataframe which only has LocationIDs \n",
    "    # match the LocationID in 'df' with dataframe 'g'\n",
    "    # add 4 new columns to 'df' after matching\n",
    "    if 'DOLocationID' in df:\n",
    "        df['pickup_latitude']  = df['PULocationID'].map(lat_map)\n",
    "        df['pickup_longitude'] = df['PULocationID'].map(lon_map)\n",
    "        df['dropoff_latitude'] = df['DOLocationID'].map(lat_map)\n",
    "        df['dropoff_longitude']= df['DOLocationID'].map(lon_map)\n",
    "        \n",
    "     # remove trips if start and/or end outside New York area\n",
    "    df = df[df['pickup_latitude'] >= NEW_YORK_BOX_COORDS[0][0]]\n",
    "    df = df[df['pickup_latitude'] <= NEW_YORK_BOX_COORDS[1][0]]\n",
    "    df = df[df['pickup_longitude'] >= NEW_YORK_BOX_COORDS[0][1]]\n",
    "    df = df[df['pickup_longitude'] <= NEW_YORK_BOX_COORDS[1][1]]\n",
    "        \n",
    "    df = df[df['dropoff_latitude'] >= NEW_YORK_BOX_COORDS[0][0]]\n",
    "    df = df[df['dropoff_latitude'] <= NEW_YORK_BOX_COORDS[1][0]]\n",
    "    df = df[df['dropoff_longitude'] >= NEW_YORK_BOX_COORDS[0][1]]\n",
    "    df = df[df['dropoff_longitude'] <= NEW_YORK_BOX_COORDS[1][1]]\n",
    "    \n",
    "    # normalize column names\n",
    "    df = df.rename(columns={'tpep_pickup_datetime':'pickup_datetime', \n",
    "                            'Trip_Pickup_DateTime':'pickup_datetime', \n",
    "                            'tpep_dropoff_datetime':'dropoff_datetime',\n",
    "                            'Fare_Amt':'fare_amount'         \n",
    "                            })\n",
    "        \n",
    "    \n",
    "    df = df[(df['fare_amount'] >0)]     \n",
    "    df['pickup_datetime'] = pd.to_datetime(df['pickup_datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "    df['dropoff_datetime'] = pd.to_datetime(df['dropoff_datetime'], format='%Y-%m-%d %H:%M:%S')\n",
    "    \n",
    "    \n",
    "    \n",
    "    #df['distance_km'] = df.apply(lambda row: calculate_distance_with_zones(row['PULocationID'], row['DOLocationID']), axis=1)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "35c9c0cd",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_and_clean_taxi_data(parquet_urls):\n",
    "    all_taxi_dataframes = []\n",
    "    \n",
    "    for parquet_url in parquet_urls:\n",
    "        print(f\"working on {parquet_url}\")\n",
    "        # maybe: first try to see if you've downloaded this exact\n",
    "        # file already and saved it before trying again\n",
    "        if os.path.exists(\"somefilename...\" ):\n",
    "            dataframe.from_csv(\"somefilename\")\n",
    "        else:\n",
    "            dataframe = get_and_clean_month(parquet_url)\n",
    "            dataframe.to_csv(\"soemfilename...\")\n",
    "    \n",
    "        # maybe: if the file hasn't been saved, save it so you can\n",
    "        # avoid re-downloading it if you re-run the function\n",
    "        add_distance_column(dataframe)\n",
    "        \n",
    "        all_taxi_dataframes.append(dataframe)\n",
    "        \n",
    "    # create one gigantic dataframe with data from every month needed\n",
    "    taxi_data = pd.concat(all_taxi_dataframes)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "200776ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_taxi_data():\n",
    "    # download files to the directory \"yellow taxi\" if files do not exist\n",
    "    all_urls = get_all_urls_from_taxi_page(TAXI_URL)\n",
    "    all_parquet_urls = filter_taxi_parquet_urls(all_urls)\n",
    "    all_files = []\n",
    "    for url in all_parquet_urls:\n",
    "        print(f\"working on {url}\")\n",
    "        response = requests.get(url, stream=True)\n",
    "        file_name = url.split(\"/\")[-1]\n",
    "        local_file_path = os.path.join(\"yellow taxi\", file_name)\n",
    "        all_files.append(local_file_path)\n",
    "        \n",
    "        if not os.path.exists(local_file_path):\n",
    "            with open(local_file_path, \"wb\") as file:\n",
    "                file.write(response.content)\n",
    "            print(f\"done with {url}\")\n",
    "    \n",
    "#     print(all_files)\n",
    "    taxi_data = get_and_clean_taxi_data(all_files)\n",
    "    return taxi_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "1c1028d1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-01.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-02.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-03.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-04.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-05.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2015-06.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-01.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-02.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-03.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-04.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-05.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-06.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-07.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-08.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-09.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-10.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-11.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2014-12.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-01.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-02.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-03.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-04.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-05.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-06.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-07.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-08.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-09.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-10.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-11.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2013-12.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-01.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-02.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-03.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-04.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-05.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-06.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-07.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-08.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-09.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-10.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-11.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2012-12.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-01.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-02.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-03.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-04.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-05.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-06.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-07.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-08.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-09.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-10.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-11.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2011-12.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-01.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-02.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-03.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-04.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-05.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-06.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-07.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-08.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-09.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-10.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-11.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2010-12.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-01.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-02.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-03.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-04.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-05.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-06.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-07.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-08.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-09.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-10.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-11.parquet\n",
      "working on https://d37ci6vzurychx.cloudfront.net/trip-data/yellow_tripdata_2009-12.parquet\n",
      "working on yellow taxi/yellow_tripdata_2015-01.parquet\n",
      "working on yellow taxi/yellow_tripdata_2015-02.parquet\n",
      "working on yellow taxi/yellow_tripdata_2015-03.parquet\n",
      "working on yellow taxi/yellow_tripdata_2015-04.parquet\n",
      "working on yellow taxi/yellow_tripdata_2015-05.parquet\n",
      "working on yellow taxi/yellow_tripdata_2015-06.parquet\n",
      "working on yellow taxi/yellow_tripdata_2014-01.parquet\n",
      "working on yellow taxi/yellow_tripdata_2014-02.parquet\n",
      "working on yellow taxi/yellow_tripdata_2014-03.parquet\n",
      "working on yellow taxi/yellow_tripdata_2014-04.parquet\n",
      "working on yellow taxi/yellow_tripdata_2014-05.parquet\n",
      "working on yellow taxi/yellow_tripdata_2014-06.parquet\n",
      "working on yellow taxi/yellow_tripdata_2014-07.parquet\n",
      "working on yellow taxi/yellow_tripdata_2014-08.parquet\n",
      "working on yellow taxi/yellow_tripdata_2014-09.parquet\n",
      "working on yellow taxi/yellow_tripdata_2014-10.parquet\n",
      "working on yellow taxi/yellow_tripdata_2014-11.parquet\n",
      "working on yellow taxi/yellow_tripdata_2014-12.parquet\n",
      "working on yellow taxi/yellow_tripdata_2013-01.parquet\n",
      "working on yellow taxi/yellow_tripdata_2013-02.parquet\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "working on yellow taxi/yellow_tripdata_2013-03.parquet\n",
      "working on yellow taxi/yellow_tripdata_2013-04.parquet\n",
      "working on yellow taxi/yellow_tripdata_2013-05.parquet\n",
      "working on yellow taxi/yellow_tripdata_2013-06.parquet\n",
      "working on yellow taxi/yellow_tripdata_2013-07.parquet\n",
      "working on yellow taxi/yellow_tripdata_2013-08.parquet\n",
      "working on yellow taxi/yellow_tripdata_2013-09.parquet\n",
      "working on yellow taxi/yellow_tripdata_2013-10.parquet\n",
      "working on yellow taxi/yellow_tripdata_2013-11.parquet\n",
      "working on yellow taxi/yellow_tripdata_2013-12.parquet\n",
      "working on yellow taxi/yellow_tripdata_2012-01.parquet\n",
      "working on yellow taxi/yellow_tripdata_2012-02.parquet\n",
      "working on yellow taxi/yellow_tripdata_2012-03.parquet\n",
      "working on yellow taxi/yellow_tripdata_2012-04.parquet\n",
      "working on yellow taxi/yellow_tripdata_2012-05.parquet\n",
      "working on yellow taxi/yellow_tripdata_2012-06.parquet\n",
      "working on yellow taxi/yellow_tripdata_2012-07.parquet\n",
      "working on yellow taxi/yellow_tripdata_2012-08.parquet\n",
      "working on yellow taxi/yellow_tripdata_2012-09.parquet\n",
      "working on yellow taxi/yellow_tripdata_2012-10.parquet\n",
      "working on yellow taxi/yellow_tripdata_2012-11.parquet\n",
      "working on yellow taxi/yellow_tripdata_2012-12.parquet\n",
      "working on yellow taxi/yellow_tripdata_2011-01.parquet\n",
      "working on yellow taxi/yellow_tripdata_2011-02.parquet\n",
      "working on yellow taxi/yellow_tripdata_2011-03.parquet\n",
      "working on yellow taxi/yellow_tripdata_2011-04.parquet\n",
      "working on yellow taxi/yellow_tripdata_2011-05.parquet\n",
      "working on yellow taxi/yellow_tripdata_2011-06.parquet\n",
      "working on yellow taxi/yellow_tripdata_2011-07.parquet\n",
      "working on yellow taxi/yellow_tripdata_2011-08.parquet\n",
      "working on yellow taxi/yellow_tripdata_2011-09.parquet\n",
      "working on yellow taxi/yellow_tripdata_2011-10.parquet\n",
      "working on yellow taxi/yellow_tripdata_2011-11.parquet\n",
      "working on yellow taxi/yellow_tripdata_2011-12.parquet\n",
      "working on yellow taxi/yellow_tripdata_2010-01.parquet\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'PULocationID', 'DOLocationID'] not in index\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m/var/folders/zj/x5dt8zs91zjdtlh4krxv9g_80000gn/T/ipykernel_85657/3210650624.py\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mtaxi_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_taxi_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/var/folders/zj/x5dt8zs91zjdtlh4krxv9g_80000gn/T/ipykernel_85657/508366686.py\u001b[0m in \u001b[0;36mget_taxi_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     17\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     18\u001b[0m \u001b[0;31m#     print(all_files)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 19\u001b[0;31m     \u001b[0mtaxi_data\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_and_clean_taxi_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mall_files\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     20\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mtaxi_data\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/zj/x5dt8zs91zjdtlh4krxv9g_80000gn/T/ipykernel_85657/684977289.py\u001b[0m in \u001b[0;36mget_and_clean_taxi_data\u001b[0;34m(parquet_urls)\u001b[0m\n\u001b[1;32m      9\u001b[0m             \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfrom_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"somefilename\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     10\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 11\u001b[0;31m             \u001b[0mdataframe\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_and_clean_month\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mparquet_url\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     12\u001b[0m             \u001b[0mdataframe\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m\"soemfilename...\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/var/folders/zj/x5dt8zs91zjdtlh4krxv9g_80000gn/T/ipykernel_85657/1834134937.py\u001b[0m in \u001b[0;36mget_and_clean_month\u001b[0;34m(url)\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      6\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_parquet\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0murl\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 7\u001b[0;31m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m'tpep_pickup_datetime'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'tpep_dropoff_datetime'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'PULocationID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'DOLocationID'\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m'fare_amount'\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      8\u001b[0m     \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdropna\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      9\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/frame.py\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   3462\u001b[0m             \u001b[0;32mif\u001b[0m \u001b[0mis_iterator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3463\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3464\u001b[0;31m             \u001b[0mindexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mloc\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_get_listlike_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3465\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3466\u001b[0m         \u001b[0;31m# take() does not accept boolean indexers\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_get_listlike_indexer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1312\u001b[0m             \u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mnew_indexer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0max\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_reindex_non_unique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1313\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1314\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_validate_read_indexer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkeyarr\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mindexer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1315\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1316\u001b[0m         if needs_i8_conversion(ax.dtype) or isinstance(\n",
      "\u001b[0;32m~/opt/anaconda3/lib/python3.8/site-packages/pandas/core/indexing.py\u001b[0m in \u001b[0;36m_validate_read_indexer\u001b[0;34m(self, key, indexer, axis)\u001b[0m\n\u001b[1;32m   1375\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1376\u001b[0m             \u001b[0mnot_found\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mensure_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mmissing_mask\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnonzero\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1377\u001b[0;31m             \u001b[0;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34mf\"{not_found} not in index\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1378\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1379\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"['tpep_pickup_datetime', 'tpep_dropoff_datetime', 'PULocationID', 'DOLocationID'] not in index\""
     ]
    }
   ],
   "source": [
    "taxi_data = get_taxi_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10ebd75e",
   "metadata": {},
   "outputs": [],
   "source": [
    "taxi_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "094b4d6d",
   "metadata": {},
   "source": [
    "### Processing Uber Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7c58e3a2",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_uber_data(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    df = df.dropna(how='any')      #remove missing values\n",
    "    df = df.drop(columns=['Unnamed: 0','key','passenger_count'])      #remove unnecessary columns\n",
    "    df = df.loc[df.pickup_longitude.between(-74.242330,-73.717047) & df.dropoff_longitude.between(-74.242330,-73.717047) \n",
    "         & df.pickup_latitude.between(40.560445,40.908524)& df.dropoff_latitude.between(40.560445,40.908524)]\n",
    "          # remove locations out of range\n",
    "    df.reset_index(drop=True, inplace=True)    #reset index\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f836f118",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_uber_data():\n",
    "    uber_dataframe = load_and_clean_uber_data(UBER_DATA)\n",
    "    uber_dataframe = add_distance_column(uber_dataframe)\n",
    "    uber_dataframe[\"pickup_datetime\"] = pd.to_datetime(uber_dataframe['pickup_datetime'])\n",
    "    uber_dataframe[\"year\"] = uber_dataframe[\"pickup_datetime\"].dt.year\n",
    "    uber_dataframe[\"month\"] = uber_dataframe[\"pickup_datetime\"].dt.month\n",
    "    uber_dataframe[\"day\"] = uber_dataframe[\"pickup_datetime\"].dt.day\n",
    "    uber_dataframe[\"hour\"] = uber_dataframe[\"pickup_datetime\"].dt.hour\n",
    "    uber_dataframe[\"dayofweek\"] = uber_dataframe[\"pickup_datetime\"].dt.dayofweek\n",
    "    return uber_dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9c2bd13f",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data = get_uber_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "339997e2",
   "metadata": {},
   "outputs": [],
   "source": [
    "uber_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45a15cbb",
   "metadata": {},
   "source": [
    "### Processing Weather Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0ec5370f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_all_weather_csvs(directory):\n",
    "    csv_files = [file for file in os.listdir(directory) if file.endswith(\".csv\")]\n",
    "    return csv_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d990317",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all the \"weather\" csv files from 2009-2015\n",
    "get_all_weather_csvs(WEATHER_CSV_DIR)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76e864ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_hourly(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    # remove columns that have all NaN values\n",
    "    df=df.dropna(axis=1, how=\"all\")\n",
    "    \n",
    "    # for \"hourly wind speed\", fill missing value (calculate hourly-data from daily-data)\n",
    "    if df['HourlyWindSpeed'] is 'NaN':\n",
    "        df['HourlyWindSpeed'] = df['DailyAverageWindSpeed']/24\n",
    "        df['HourlyWindSpeed'].apply(lambda x: round(x,2))\n",
    "        \n",
    "    # only keep necessary columns\n",
    "    keep = ['DATE', 'HourlyPrecipitation', 'HourlyWindSpeed']\n",
    "    \n",
    "    #in \"hourly precipation\" column, replace \"T\" with value 0, replace NaN with 0 since existing value \n",
    "    # for the column is small\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].replace({pd.np.nan: 0, 'T': 0})\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].apply(  lambda x: x.replace('s', '') \n",
    "                                                                if isinstance(x, str) else x)\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].astype(float)\n",
    "    \n",
    "    df1 = pd.DataFrame()\n",
    "    for i in keep:\n",
    "        col1=df.loc[:,df.columns.str.contains(i)]\n",
    "        df1=pd.concat([df1, col1], axis=1)\n",
    "    hourly_clean= df1.dropna()\n",
    "    hourly_clean.reset_index(drop=True, inplace=True)\n",
    "    \n",
    "    # Add columns of year, month, day and hour\n",
    "    hourly_clean[\"DATE\"] = pd.to_datetime(hourly_clean['DATE'])\n",
    "    hourly_clean[\"YEAR\"] = hourly_clean[\"DATE\"].dt.year\n",
    "    hourly_clean[\"MONTH\"] = hourly_clean[\"DATE\"].dt.month\n",
    "    hourly_clean[\"DAY\"] = hourly_clean[\"DATE\"].dt.day\n",
    "    hourly_clean[\"HOUR\"] = hourly_clean[\"DATE\"].dt.hour\n",
    "    hourly_clean['HourlyPrecipitation'] = hourly_clean['HourlyPrecipitation'].round(2)\n",
    "    hourly_clean.dropna(inplace=True)\n",
    "    return hourly_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "301e093b",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_month_weather_data_hourly('weather/2012_weather.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0687581f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_month_weather_data_daily(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "    \n",
    "    #remove columns having all NaN values\n",
    "    df = df.dropna(axis=1, how=\"all\")\n",
    "    \n",
    "    #in \"hourly precipation\" column, replace \"T\" with value 0, replace NaN with 0 since existing value \n",
    "    # for the column is small\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].replace({pd.np.nan: 0, 'T': 0})\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].apply(  lambda x: x.replace('s', '') \n",
    "                                                                if isinstance(x, str) else x)\n",
    "    df['HourlyPrecipitation'] = df['HourlyPrecipitation'].astype(float)\n",
    "    \n",
    "    df['DATE'] = pd.to_datetime(df['DATE']) \n",
    "    df['DATE'] = df['DATE'].apply(lambda x: x.date())\n",
    "    \n",
    "    # Group by same date, calculate the mean,use that number as daily data\n",
    "    Daily_w = df.groupby('DATE')['HourlyWindSpeed'].mean()\n",
    "    Daily_p = df.groupby('DATE')['HourlyPrecipitation'].mean()\n",
    "    #df1 = df.drop_duplicates(subset=['DATE'])\n",
    "    df = df[['DATE']]\n",
    "    df = pd.merge(df, Daily_w, on='DATE')\n",
    "    df= pd.merge(df, Daily_p, on='DATE')\n",
    "    \n",
    "    df.rename(columns={'HourlyWindSpeed':'DailyWindSpeed','HourlyPrecipitation':'DailyPrecipitation'},inplace=True)\n",
    "    \n",
    "    df['DailyWindSpeed'] = df['DailyWindSpeed'].round(2)\n",
    "    df['DailyPrecipitation'] = df['DailyPrecipitation'].round(2)\n",
    "    \n",
    "    # Add columns of year, month, day and hour\n",
    "    df[\"DATE\"] = pd.to_datetime(df['DATE'])\n",
    "    df[\"YEAR\"] = df[\"DATE\"].dt.year\n",
    "    df[\"MONTH\"] = df[\"DATE\"].dt.month\n",
    "    df[\"DAY\"] = df[\"DATE\"].dt.day\n",
    "    df[\"HOUR\"] = df[\"DATE\"].dt.hour\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "610c932a",
   "metadata": {},
   "outputs": [],
   "source": [
    "clean_month_weather_data_daily('weather/2012_weather.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "af7ca370",
   "metadata": {},
   "source": [
    "#### Extra_credit for sunrise dataset:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bfcf3b43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_weather_data_dailysunrise(csv_file):\n",
    "    df = pd.read_csv(csv_file)\n",
    "\n",
    "    titles = ['DATE', 'Sunrise', 'Sunset']\n",
    "    df_sunrise = pd.DataFrame()\n",
    "    for i in titles:\n",
    "        colms = df.loc[:,df.columns.str.contains(i)]\n",
    "        df_sunrise = pd.concat([df_sunrise, colms], axis=1)\n",
    "    sunrise_clean = df_sunrise.dropna()\n",
    "    sunrise_clean.reset_index(drop=True, inplace=True)\n",
    "    # Add the columns of year, month, day and hour according to the date\n",
    "    sunrise_clean[\"DATE\"] = pd.to_datetime(sunrise_clean['DATE'])\n",
    "    sunrise_clean[\"YEAR\"] = sunrise_clean[\"DATE\"].dt.year\n",
    "    sunrise_clean[\"MONTH\"] = sunrise_clean[\"DATE\"].dt.month\n",
    "    sunrise_clean[\"DAY\"] = sunrise_clean[\"DATE\"].dt.day\n",
    "    sunrise_clean[\"HOUR\"] = sunrise_clean[\"DATE\"].dt.hour\n",
    "    # Add the columns of hour and minutes according to the sunrise time\n",
    "    sunrise_clean[\"Sunrise\"] = pd.to_datetime(sunrise_clean[\"Sunrise\"],format='%H%M')\n",
    "    sunrise_clean[\"Sunrise_hour\"] = sunrise_clean[\"Sunrise\"].dt.hour\n",
    "    sunrise_clean[\"Sunrise_minute\"] = sunrise_clean[\"Sunrise\"].dt.minute\n",
    "    # Add the columns of hour and minutes according to the sunset time\n",
    "    sunrise_clean[\"Sunset\"] = pd.to_datetime(sunrise_clean[\"Sunset\"],format='%H%M')\n",
    "    sunrise_clean[\"Sunset_hour\"] = sunrise_clean[\"Sunset\"].dt.hour\n",
    "    sunrise_clean[\"Sunset_minute\"] = sunrise_clean[\"Sunset\"].dt.minute\n",
    "    return sunrise_clean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ef8945d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_and_clean_weather_data():\n",
    "    weather_csv_files = get_all_weather_csvs(WEATHER_CSV_DIR)\n",
    "    \n",
    "    hourly_dataframes = []\n",
    "    daily_dataframes = []\n",
    "    sunrise_dataframes = []\n",
    "        \n",
    "    for csv_file in weather_csv_files:\n",
    "        hourly_dataframe = clean_month_weather_data_hourly(WEATHER_CSV_DIR+\"/\"+csv_file)\n",
    "        daily_dataframe = clean_month_weather_data_daily(WEATHER_CSV_DIR+\"/\"+csv_file)\n",
    "        sunrise_dataframe = clean_weather_data_dailysunrise(WEATHER_CSV_DIR+\"/\"+csv_file)\n",
    "        hourly_dataframes.append(hourly_dataframe)\n",
    "        daily_dataframes.append(daily_dataframe)\n",
    "        sunrise_dataframes.append(sunrise_dataframe)\n",
    "        \n",
    "    # create two dataframes with hourly & daily data from every month\n",
    "    hourly_data = pd.concat(hourly_dataframes)\n",
    "    daily_data = pd.concat(daily_dataframes)\n",
    "    sunrise_data = pd.concat(sunrise_dataframes)\n",
    "    \n",
    "    return hourly_data, daily_data, sunrise_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f7cd53a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data, daily_weather_data, sunrise_data = load_and_clean_weather_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48216557",
   "metadata": {},
   "outputs": [],
   "source": [
    "hourly_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4cb386ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_weather_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4e7a771e",
   "metadata": {},
   "outputs": [],
   "source": [
    "sunrise_data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd101f11",
   "metadata": {},
   "source": [
    "## Part 2: Storing Cleaned Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3529cf6",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine = db.create_engine(DATABASE_URL)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d2bea0ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# if using SQL (as opposed to SQLAlchemy), define the commands \n",
    "# to create your 4 tables/dataframes\n",
    "HOURLY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS hourly_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATE,\n",
    "    HourlyPrecipitation FLOAT,\n",
    "    HourlyWindSpeed FLOAT,\n",
    "    YEAR INTEGER,\n",
    "    MONTH INTEGER,\n",
    "    DAY INTEGER,\n",
    "    HOUR INTEGER\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "DAILY_WEATHER_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS daily_weather\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE DATE,\n",
    "    DailyWindSpeed FLOAT,\n",
    "    DailyPrecipitation FLOAT,\n",
    "    YEAR INTEGER,\n",
    "    MONTH INTEGER,\n",
    "    DAY INTEGER,\n",
    "    HOUR INTEGER\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "TAXI_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS taxi_trips (\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    pickup_datetime DATE,\n",
    "    dropoff_datetime DATE,\n",
    "    PULocationID INTEGER,\n",
    "    DOLocationID INTEGER,\n",
    "    fare_amount FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    pickup_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    distance FLOAT\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "UBER_TRIPS_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS uber_trips\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    fare_amount FLOAT,\n",
    "    pickup_datetime DATETIME,\n",
    "    pickup_longitude FLOAT,\n",
    "    pickup_latitude FLOAT,\n",
    "    dropoff_longitude FLOAT,\n",
    "    dropoff_latitude FLOAT,\n",
    "    distance FLOAT,\n",
    "    year INTEGER,\n",
    "    month INTEGER,\n",
    "    day INTEGER,\n",
    "    hour INTEGER,\n",
    "    dayofweek INTEGER\n",
    ");\n",
    "\"\"\"\n",
    "\n",
    "SUNRISE_SCHEMA = \"\"\"\n",
    "CREATE TABLE IF NOT EXISTS sunrise_data\n",
    "(\n",
    "    id INTEGER PRIMARY KEY,\n",
    "    DATE TEXT,\n",
    "    Sunrise TEXT,\n",
    "    Sunset TEXT,\n",
    "    YEAR INTEGER,\n",
    "    MONTH INTEGER,\n",
    "    DAY INTEGER,\n",
    "    HOUR INTEGER,\n",
    "    Sunrise_Hour INTEGER,\n",
    "    Sunrise_Minute INTEGER,\n",
    "    Sunset_Hour INTEGER,\n",
    "    Sunset_Minute INTEGER\n",
    ")\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5f41e54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create that required schema.sql file\n",
    "with open(DATABASE_SCHEMA_FILE, \"w\") as f:\n",
    "    f.write(HOURLY_WEATHER_SCHEMA)\n",
    "    f.write(DAILY_WEATHER_SCHEMA)\n",
    "    f.write(TAXI_TRIPS_SCHEMA)\n",
    "    f.write(UBER_TRIPS_SCHEMA)\n",
    "    f.write(SUNRISE_SCHEMA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02eccdba",
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the tables with the schema files\n",
    "with engine.connect() as connection:\n",
    "    afile = open(DATABASE_SCHEMA_FILE, 'r')\n",
    "    sqlFile = afile.read()\n",
    "    afile.close()\n",
    "\n",
    "    sqlQuery = sqlFile.split(';')\n",
    "    for query in sqlQuery:\n",
    "        connection.execute(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c122964f",
   "metadata": {},
   "source": [
    "### Add Data to Database"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e68a363",
   "metadata": {},
   "outputs": [],
   "source": [
    "def write_dataframes_to_table(table_to_df_dict):\n",
    "    # convert all pd DateFrames to sql tables\n",
    "    for table, df in table_to_df_dict.items():\n",
    "        df.to_sql(table, engine,if_exists='append', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "45d6c06c",
   "metadata": {},
   "outputs": [],
   "source": [
    "map_table_name_to_dataframe = {\n",
    "    \"taxi_trips\": taxi_data,\n",
    "    \"uber_trips\": uber_data,\n",
    "    \"hourly_weather\": hourly_weather_data,\n",
    "    \"daily_weather\": daily_weather_data,\n",
    "    \"sunrise_data\": sunrise_data,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "74004f96",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_dataframes_to_table(map_table_name_to_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8cb6e33e",
   "metadata": {},
   "source": [
    "## Part 3: Understanding the Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a849e92",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Helper function to write the queries to file\n",
    "def write_query_to_file(query, outfile):\n",
    "    with open(QUERY_DIRECTORY + outfile, \"w\") as f:\n",
    "        f.write(query)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ee70a777",
   "metadata": {},
   "source": [
    "### Query 1:For 01-2009 through 06-2015, show the popularity of Yellow Taxi rides for each hour of the day. The query result should have 24 bins, one for each hour, descending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db871d3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_1_FILENAME = \"Hourly_popularity_Yellow_Taxi.sql\"\n",
    "\n",
    "QUERY_1 = \"\"\"\n",
    "SELECT COUNT(id) AS number,\n",
    "strftime('%H', pickup_datetime) AS time\n",
    "FROM taxi_trips\n",
    "WHERE pickup_datetime BETWEEN '2009-01-01' AND '2015-06-31'\n",
    "GROUP BY time\n",
    "ORDER BY number DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5275f3a",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_1).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2ef04df",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_1, QUERY_1_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2c7df5e",
   "metadata": {},
   "source": [
    "### Query 2: For the same time frame, show the popularity of Uber rides for each day of the week.The result should have 7 bins, one for each day, descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "565c7f64",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_2_FILENAME = \"Day_popularity_Uber.sql\"\n",
    "\n",
    "QUERY_2 = \"\"\"\n",
    "SELECT COUNT(id) AS number,\n",
    "strftime('%w', pickup_datetime) AS time\n",
    "FROM uber_trips\n",
    "WHERE pickup_datetime BETWEEN '2009-01-01' AND '2015-06-31'\n",
    "GROUP BY time\n",
    "ORDER BY number DESC\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cd242e",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_2).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b58ad9e",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_2, QUERY_2_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75c408b5",
   "metadata": {},
   "source": [
    "### Query 3: What is the 95% percentile of distance traveled for all hired trips during July 2013? The result should be a float. Itâ€™s okay if itâ€™s a single float within a list and/or tuple.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47288658",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_3_FILENAME = \"July_2013_95percent_distances.sql\"\n",
    "\n",
    "QUERY_3 = \"\"\"\n",
    "WITH all_trips AS(\n",
    "    SELECT pickup_datetime, distance\n",
    "    FROM taxi_trips\n",
    "    WHERE pickup_datetime BETWEEN '2013-07-01' AND '2013-08-01'\n",
    "    UNION ALL\n",
    "    SELECT pickup_datetime, distance\n",
    "    FROM uber_trips\n",
    "    WHERE pickup_datetime BETWEEN '2013-07-01' AND '2013-08-01'\n",
    ")\n",
    "SELECT distance\n",
    "FROM all_trips\n",
    "ORDER BY distance \n",
    "LIMIT 1\n",
    "OFFSET (SELECT COUNT(*) FROM all_trips) * 95 / 100 - 1\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "96de3644",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_3).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a724415d",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_3, QUERY_3_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d0ea818",
   "metadata": {},
   "source": [
    "### Query 4:  What were the top 10 days with the highest number of hired rides for 2009, and what was the average distance for each day? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c4e8d89f",
   "metadata": {},
   "source": [
    "#### The result should be a list of 10 tuples. Each tuple should have three items: a date, an integer for the number of rides of that date, and a float for the average distance of that date. The list of tuples should be sorted by total number of rides, descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9d2f070",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_4_FILENAME = \"2009_top_10_days.sql\"\n",
    "\n",
    "QUERY_4 = \"\"\"\n",
    "WITH all_trips AS (\n",
    "    SELECT pickup_datetime, distance\n",
    "    FROM taxi_trips\n",
    "    WHERE pickup_datetime BETWEEN '2009-01-01' AND '2010-01-01'\n",
    "    UNION ALL\n",
    "    SELECT pickup_datetime, distance\n",
    "    FROM uber_trips\n",
    "    WHERE pickup_datetime BETWEEN '2009-01-01' AND '2010-01-01'\n",
    ")\n",
    "SELECT DATE(pickup_datetime) AS day,\n",
    "AVG(distance) AS average_dis,\n",
    "COUNT(*) AS number\n",
    "FROM all_trips\n",
    "GROUP BY DATE(pickup_datetime)\n",
    "ORDER BY number DESC\n",
    "LIMIT 10\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d53ccbe4",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_4).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7719f43a",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_4, QUERY_4_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ed7df16",
   "metadata": {},
   "source": [
    "### Query 5: Which 10 days in 2014 were the windiest on average, and how many hired trips were made on those days? "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6746cfe8",
   "metadata": {},
   "source": [
    "#### The result should be a list of 10 tuples. Each tuple should have three items: a date, a float for the average wind speed of that day, and the number of hired trips for that day. The list of tuples should be sorted by the average wind speed, descending."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a5be71c",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_5_FILENAME = \"2014_top_10_windiest_days.sql\"\n",
    "\n",
    "QUERY_5 = \"\"\"\n",
    "WITH all_trips AS (\n",
    "    SELECT DATE(pickup_datetime) AS pickup_date, \n",
    "    distance\n",
    "    FROM taxi_trips\n",
    "    UNION ALL\n",
    "    SELECT DATE(pickup_datetime) AS pickup_date, \n",
    "    distance\n",
    "    FROM uber_trips\n",
    "),\n",
    "\n",
    "windiest_days AS (\n",
    "    SELECT DATE(DATE) AS wind_date,\n",
    "    DailyWindSpeed\n",
    "    FROM daily_weather\n",
    "    WHERE wind_date BETWEEN '2014-01-01' AND '2015-01-01'\n",
    "    ORDER BY DailyWindSpeed DESC\n",
    "    LIMIT 10\n",
    ")\n",
    "\n",
    "SELECT w.wind_date,\n",
    "COUNT(*)\n",
    "FROM windiest_days AS w,all_trips AS a\n",
    "WHERE w.wind_date == a.pickup_date\n",
    "GROUP BY w.wind_date\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "feefbbc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_5).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ebcf0fe",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_5, QUERY_5_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65db3fcb",
   "metadata": {},
   "source": [
    "### Query 6: During Hurricane Sandy in NYC (Oct 29-30, 2012), plus the week leading up and the week after, how many trips were taken each hour, and for each hour, how much precipitation did NYC receive and what was the sustained wind speed? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c8a0431",
   "metadata": {},
   "source": [
    "#### The result should be a list of roughly 384 tuples, where each tuple is an entry for every single hour of the given date range, even if no rides were taken, no precipitation was measured, or there was no wind. Each tuple should have four items: a string for the date and hour, an int for the number of hired rides in that hour, the float for the total precipitation for that hour, and a float for the average wind speed for that hour. The list of tuples should be ordered by date+hour, ascending.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfb6e2c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "QUERY_6_FILENAME = \"Hurricane_Sandy_in_NYC.sql\"\n",
    "\n",
    "QUERY_6 = \"\"\"\n",
    "WITH hourly_data AS (\n",
    "SELECT strftime ('%Y %m %d %H', DATE) AS date_hour, \n",
    "AVG(HourlyWindSpeed) AS hourly_wind_speed, \n",
    "AVG(HourlyPrecipitation) AS hourly_precipitation \n",
    "FROM hourly_weather\n",
    "WHERE DATE BETWEEN '2012-10-22' AND '2012-11-05'\n",
    "GROUP BY date_hour ),\n",
    "\n",
    "hired_trips AS (\n",
    "SELECT strftime ('%Y %m %d %H', pickup_datetime) AS pick_up_date_hour,\n",
    "    distance\n",
    "    FROM taxi_trips\n",
    "    WHERE pickup_datetime BETWEEN '2012-10-22' AND '2012-11-05'\n",
    "    UNION ALL\n",
    "    SELECT strftime ('%Y %m %d %H', pickup_datetime) AS pick_up_date_hour,\n",
    "    distance\n",
    "    FROM uber_trips\n",
    "    WHERE pickup_datetime BETWEEN '2012-10-22' AND '2012-11-05'\n",
    ")\n",
    "\n",
    "SELECT hd.date_hour as date, \n",
    "COALESCE(COUNT(ht.pick_up_date_hour),0) AS num, \n",
    "hourly_wind_speed, \n",
    "hourly_precipitation\n",
    "FROM hourly_data AS hd\n",
    "LEFT JOIN hired_trips AS ht\n",
    "ON hd.date_hour = ht.pick_up_date_hour\n",
    "GROUP BY hd.date_hour\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3394d824",
   "metadata": {},
   "outputs": [],
   "source": [
    "engine.execute(QUERY_6).fetchall()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "24d2c3c6",
   "metadata": {},
   "outputs": [],
   "source": [
    "write_query_to_file(QUERY_6, QUERY_6_FILENAME)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2ce294ac",
   "metadata": {},
   "source": [
    "#### Extra_credit : Add an additional SQL table that contains daily sunset/sunrise data (which can be found in the original weather data). Write a question that considers this new table in relation to one or more existing tables, and then create either a SQL query or a visualization that answers that question."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d53a75d4",
   "metadata": {},
   "source": [
    "##### Question: Compare the difference of tips for trips that before and after sunset "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dffaeec",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Join taxi data and sunset data using the year, month and day\n",
    "query = \"\"\"\n",
    "        SELECT Sunrise_hour +1 AS Sunrise_hour,\n",
    "        Sunrise_minute,\n",
    "        t.hour +1 AS Pickup_Hour,\n",
    "        t.minute AS Pickup_minute, \n",
    "        tip\n",
    "        FROM sunrise_data AS s\n",
    "        JOIN taxi_trips AS t \n",
    "        ON s.month = t.month AND s.day = t.day AND s.year = t.year \n",
    "        \"\"\"\n",
    "sunrise1 = pd.read_sql(query, engine)\n",
    "sunrise.dropna()\n",
    "#filter out the trips after sunrise\n",
    "sunrise2 = sunrise1.loc[sunrise1[\"Pickup_Hour\"] >= sunrise1[\"Sunrise_hour\"]]\n",
    "after_sunrise = sunrise2.loc[sunrise2[\"Pickup_minute\"] >= sunrise2[\"Sunrise_minute\"]]\n",
    "#drop the trips that after sunrise using index values and we get trips before sunrise\n",
    "after = after_sunrise.index\n",
    "before_sunrise = sunrise1.drop(after)\n",
    "#compare the average tips amount for trips that before and after sunrise\n",
    "def after_sunrise_tip_higher():\n",
    "    if before_sunset[\"tip\"].mean() < after_sunset[\"tip\"].mean():\n",
    "        return True\n",
    "    return False\n",
    "\n",
    "after_sunrise_tip_higher()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e55ed871",
   "metadata": {},
   "source": [
    "#### Conclusion:"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a13ced42",
   "metadata": {},
   "source": [
    "## Part 4: Visualizing the Data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6d9eef42",
   "metadata": {},
   "source": [
    "### Visualization 1"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1a803597",
   "metadata": {},
   "source": [
    "### Create an appropriate visualization for the first query/question in part 3: For 01-2009 through 06-2015, show the popularity of Yellow Taxi rides for each hour of the day."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0de8394c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_1(df):\n",
    "    fig = plt.figure(figsize=(8, 6))\n",
    "    x,y = df['time'],df['num']\n",
    "    plt.bar(x,y, label='bar')\n",
    "    plt.xlabel('Time in a day')\n",
    "    plt.ylabel('Number of yellow taxi rides')\n",
    "    plt.title('popularity of Yellow Taxi rides for each hour of the day from 01-2009 to 06-2015')    \n",
    "    plt.legend(frameon=True, loc=\"upper left\")\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "847ced2f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_1():\n",
    "    df = pd.read_sql_query(QUERY_1, engine)\n",
    "    df = df.sort_values('time', ascending=True)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c63e845",
   "metadata": {},
   "outputs": [],
   "source": [
    "some_dataframe = get_data_for_visual_1()\n",
    "plot_visual_1(some_dataframe)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcba1274",
   "metadata": {},
   "source": [
    "#### Extra_credit: Animation for V1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7cfec58",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_img_list():\n",
    "    df = get_data_for_visual_1()\n",
    "    img_list = []\n",
    "    for i in range(1,24):\n",
    "        fig = Figure(figsize=(10,4))\n",
    "        canvas = FigureCanvasAgg(fig)\n",
    "        dfCurr = df.iloc[:i, :]\n",
    "        x, y = dfCurr['time'], dfCurr['num']\n",
    "        ax = fig.subplots(1,1)\n",
    "        ax.bar(x, y, label='bar')\n",
    "        ax.set_xlabel('time')\n",
    "        ax.set_ylabel('Number')\n",
    "        ax.set_title('Number of Taxi Trips each hour')    \n",
    "        ax.legend(frameon=True, loc=\"upper left\")\n",
    "        canvas.draw()\n",
    "        rgba = np.asarray(canvas.buffer_rgba())\n",
    "        img_list.append(rgba)\n",
    "    return img_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54f5a596",
   "metadata": {},
   "outputs": [],
   "source": [
    "# make it an animation\n",
    "img_list = get_img_list()\n",
    "print(len(img_list))\n",
    "vidname = \"visual 1 vid\"\n",
    "fps = 16\n",
    "fig = plt.figure(figsize=(15,10))\n",
    "plt.axis(\"off\")\n",
    "ims = [[plt.imshow(i, animated=True)] for i in img_list]\n",
    "ani = animation.ArtistAnimation(fig, ims, interval=300, repeat_delay=1000,blit=True) \n",
    "# ani.save(vidname+\".mp4\",fps=16)\n",
    "HTML(ani.to_jshtml())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60d6483",
   "metadata": {},
   "source": [
    "### Visualization 2"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6af2673",
   "metadata": {},
   "source": [
    "### Create a visualization that shows the average distance traveled per month (regardless of year - so group by each month) for both taxis and Ubers combined. Include the 90% confidence interval around the mean in the visualization.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edaf6fae",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_visual_2(dataframe): \n",
    "    \n",
    "    # plot lines for taxi and uber\n",
    "    plt.plot(t['month'], t['avg_dis'], label='Taxi', color='red')\n",
    "    plt.plot(u['month'], u['avg_dis'], label='Uber', color='blue')\n",
    "    \n",
    "    # calculate 90% CI    \n",
    "    plt.fill_between(t['month'], t['avg_dis']-1.645*taxi_std, taxi['avg_dis']+1.645*taxi_std, color='red', alpha=0.1)\n",
    "    plt.fill_between(u['month'], u['avg_dis']-1.645*uber_std, uber['avg_dis']+1.645*uber_std, color='blue', alpha=0.1)\n",
    "\n",
    "    # Set x-axis, y-axis label and title\n",
    "    plt.xlabel('Month')\n",
    "    plt.ylabel('Average Distance Travelled')\n",
    "    plt.title('Average distance travelled per month with 90% CI')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3365486d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_2():\n",
    "    \n",
    "    taxi=\"\"\"SELECT strftime ('%m', pickup_datetime) AS month, AVG(distance) as avg_dis\n",
    "              FROM taxi_trips\n",
    "              GROUP BY month\n",
    "           \"\"\"\n",
    "    uber=\"\"\"SELECT strftime ('%m', pickup_datetime) AS month, AVG(distance) as avg_dis\n",
    "              FROM uber_trips\n",
    "              GROUP BY month\n",
    "           \"\"\"\n",
    "    avg_taxi_distance = pd.read_sql_query(taxi, engine)\n",
    "    avg_uber_distance = pd.read_sql_query(uber, engine)\n",
    "    return avg_taxi_distance,avg_uber_distance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "edc7da2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "t,u = get_data_for_visual_2()\n",
    "plot_visual_2(t,u)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f8292b9e",
   "metadata": {},
   "source": [
    "### Visualization 3"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3d6f712d",
   "metadata": {},
   "source": [
    "### Define three lat/long coordinate boxes around the three major New York airports: LGA, JFK, and EWR (you can use bboxfinder to help). Create a visualization that compares what day of the week was most popular for drop offs for each airport.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f592c90e",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_3():\n",
    "   \n",
    "    boxes = {\n",
    "        \"LGA\": [-73.889473,40.766102,-73.857630,40.782806],\n",
    "        \"JFK\" : [-73.826978,40.618945,-73.741319,40.673388],\n",
    "        \"EWR\" : [-74.199343,40.668791,-74.150248,40.712069]}\n",
    "    df = pd.DataFrame(index=range(7))\n",
    "    \n",
    "    for (addr, box) in boxes.items():\n",
    "        query=f\"\"\"\n",
    "        WITH data AS\n",
    "        (SELECT pickup_datetime, dropoff_latitude, dropoff_longitude FROM uber_trips \n",
    "        UNION ALL\n",
    "        SELECT pickup_datetime, dropoff_latitude, dropoff_longitude FROM taxi_trips)\n",
    "        \n",
    "        SELECT count(*) AS {addr}, strftime('%w', pickup_datetime) AS `day_of_week` \n",
    "        FROM data WHERE dropoff_longitude>={bbox[0]} and dropoff_latitude>={bbox[1]}\n",
    "        and dropoff_longitude<={bbox[2]} and dropoff_latitude<={bbox[3]} \n",
    "        GROUP BY `day_of_week`;\n",
    "        \"\"\"\n",
    "        dt = pd.read_sql_query(query, engine)\n",
    "        df = pd.concat([df, dt.iloc[:, 0]], axis=1)\n",
    "    df = pd.DataFrame(df)\n",
    "    df.index = df.index.map(lambda x: x+1)    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "31a0da76",
   "metadata": {},
   "outputs": [],
   "source": [
    "df = get_data_for_visual_3()\n",
    "\n",
    "plt.plot(df.index, df['LGA'], label='LGA')\n",
    "plt.plot(df.index, df['JFK'], label='JFK')\n",
    "plt.plot(df.index, df['EWR'], label='EWR')\n",
    "plt.legend()\n",
    "plt.xlabel('Day of week')\n",
    "plt.ylabel('Number of dropoffs')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0dc8366",
   "metadata": {},
   "source": [
    "### Visualization 4\n",
    "### Create a heatmap of all hired trips over a map of the area. Consider using Folium or KeplerGL or another library that helps generate geospatial visualizations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70de5e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_4():\n",
    "    \n",
    "    g = gpd.read_file(filename='taxi_zones.zip', engine='fiona')\n",
    "    g = g.to_crs(4326)\n",
    "    g['longitude'] = g.centroid.x\n",
    "    g['latitude'] = g.centroid.y\n",
    "    \n",
    "    # write queries to fetch latitude and longitude from trips\n",
    "    query = \"\"\"\n",
    "            SELECT pickup_latitude, \n",
    "            pickup_longitude\n",
    "            FROM uber_trips\n",
    "            UNION ALL\n",
    "            SELECT pickup_latitude, \n",
    "            pickup_longitude \n",
    "            FROM taxi_trips\n",
    "            \"\"\"\n",
    "    data = pd.read_sql_query(query, engine)\n",
    "    geoData = gpd.GeoDataFrame(data, geometry=gpd.points_from_xy(data.pickup_longitude, data.pickup_latitude))\n",
    "    data = g.sjoin(geoData, how=\"left\", predicate=\"contains\")\n",
    "    \n",
    "    # count number of rides\n",
    "    countData = data.groupby(by=[\"OBJECTID\"])[\"OBJECTID\"].count()\n",
    "    countData.reset_index(inplace=True, drop=True)\n",
    "    g = g.assign(count=countData)\n",
    "    \n",
    "    return g"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62a577f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def all_hired_trips_heatmap():\n",
    "    \n",
    "    fig, ax = plt.subplots(figsize=(15, 10))\n",
    "    df = get_data_for_visual_4()\n",
    "    df.plot(column='count', scheme=\"MaxP\", k=8, ax=ax, cmap='Pastel2',legend=True,\n",
    "            missing_kwds={\"color\": \"lightgrey\", \"edgecolor\": \"red\", \"hatch\": \"///\", \"label\": \"Missing values\"},\n",
    "            legend_kwds={\"bbox_to_anchor\" :(1.8, 1)})\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18690e33",
   "metadata": {},
   "outputs": [],
   "source": [
    "all_hired_trips_heatmap()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2281fa02",
   "metadata": {},
   "outputs": [],
   "source": [
    "heat_map = KeplerGl()\n",
    "heat_map.add_data(data=get_data_for_visual_4(), name=\"count\")\n",
    "heat_map"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16cfc451",
   "metadata": {},
   "source": [
    "### Visualization 5\n",
    "### Create a scatter plot that compares tip amount versus distance for Yellow Taxi rides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc3bf105",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_tip_distance(df):\n",
    "    df = df[df['tip_amount']>0]\n",
    "    df = df.sample(1000)\n",
    "    \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(df['distance'], df['tip_amount'])\n",
    "    \n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('Tip amount')\n",
    "    plt.title('Scatter plot of tip amount and distance')    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "676b8e76",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_5():\n",
    "    query = \"\"\"\n",
    "            SELECT tip_amount,\n",
    "            distance \n",
    "            FROM taxi_trips\n",
    "            \"\"\"\n",
    "    df = pd.read_sql_query(query, engine)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "47303aa0",
   "metadata": {},
   "outputs": [],
   "source": [
    "df5 = get_data_for_visual_5()\n",
    "scatter_tip_distance(df5)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a94a67e6",
   "metadata": {},
   "source": [
    "### Visualization 6\n",
    "### Create another scatter plot that compares tip amount versus precipitation amount for Yellow Taxi rides. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5b5f4e71",
   "metadata": {},
   "outputs": [],
   "source": [
    "def scatter_tip_precipitation(df):\n",
    "    \n",
    "    df = df[df['tip_amount']>0] # filter tips which are greater than zero \n",
    "    fig = plt.figure(figsize=(10, 8))\n",
    "    plt.scatter(df['HourlyPrecipitation'], df['tip_amount'])\n",
    "    plt.xlabel('Precipitation')\n",
    "    plt.ylabel('Tip amount')\n",
    "    plt.title('Scatter polt of tip amount and precipitation')    \n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f1f4bd14",
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_data_for_visual_6():\n",
    "    query_tip = \"\"\"\n",
    "                 SELECT tip_amount,\n",
    "                 strftime('%Y-%m-%d %H', pickup_datetime) AS date\n",
    "                 FROM taxi_trips\n",
    "                \"\"\"\n",
    "    \n",
    "    query_rain = \"\"\"\n",
    "                 SELECT HourlyPrecipitation, \n",
    "                 strftime('%Y-%m-%d %H', DATE) AS date\n",
    "                 FROM hourly_weather\n",
    "                 \"\"\"\n",
    "    df_tip = pd.read_sql_query(query_tip, engine)\n",
    "    df_rain = pd.read_sql_query(query_rain, engine)\n",
    "    df = pd.merge(df_tip, df_rain, on='date')\n",
    "    df = df.drop('date',axis=1)\n",
    "    \n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff8b2bb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "df6 = get_data_for_visual_6()\n",
    "scatter_tip_precipitation(df6)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
